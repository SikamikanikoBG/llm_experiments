{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e05ac6",
   "metadata": {},
   "source": [
    "# Ollama server parallel API calls simulation\n",
    "\n",
    "## Author: Arsen Apostolov\n",
    "LinkedIn Profile: [www.linkedin.com/in/arsenapostolov](https://www.linkedin.com/in/arsenapostolov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d00a5",
   "metadata": {},
   "source": [
    "## Parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333aee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama setup\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "ollama_model = \"tinyllama\"\n",
    "ollama_prompt = \"Why is the sky blue?\"\n",
    "\n",
    "# How many simulations to be done. Start from initial calls and increment till final calls or timeout of Ollama reached\n",
    "initial_n_calls = 1\n",
    "final_n_calls = 20\n",
    "increment_n_calls = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6dfda",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ed436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from threading import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f96898",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = \"ollama_responses.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the workbook and sheet\n",
    "def setup_workbook(log_file_path):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Responses\"\n",
    "    columns = [\"ID\", \"Time\", \"Speed\", \"Waiting Time\", \"Response\"]\n",
    "    for col_num, column_title in enumerate(columns, 1):\n",
    "        ws.cell(row=1, column=col_num, value=column_title)\n",
    "    wb.save(log_file_path)\n",
    "\n",
    "# Call this function at the start of your script or before you start logging\n",
    "setup_workbook(log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a lock\n",
    "lock = Lock()\n",
    "\n",
    "def log_response(call_id, response_text, speed_tokens_per_second, waiting_time):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    wb = openpyxl.load_workbook(log_file_path)  # Open the workbook\n",
    "    ws = wb.active\n",
    "    ws.append([call_id, timestamp, speed_tokens_per_second, waiting_time, response_text])\n",
    "    wb.save(log_file_path)  # Save the workbook\n",
    "    wb.close()  # Close the workbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34fe1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama_model(call_id):\n",
    "    data = {\n",
    "        \"model\": ollama_model,\n",
    "        \"prompt\": ollama_prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    start_call_time = time.time()  # Time before the request is made\n",
    "    waiting_time = None  # Initialize waiting_time as None\n",
    "\n",
    "    try:\n",
    "        response = requests.post(ollama_url, json=data, timeout=30)\n",
    "        end_call_time = time.time()  # Time after the response is received\n",
    "        waiting_time = end_call_time - start_call_time\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            if response_data.get('done', False):\n",
    "                eval_duration_ns = response_data.get('eval_duration', 0)\n",
    "                eval_count = response_data.get('eval_count', 0)\n",
    "                speed_tokens_per_second = eval_count / (eval_duration_ns / 1e9) if eval_duration_ns > 0 else 0\n",
    "                response_text = response_data.get('response', 'No response')\n",
    "                log_response(call_id, response_text, speed_tokens_per_second, waiting_time)  # Log with speed and waiting time\n",
    "                return speed_tokens_per_second, waiting_time\n",
    "            else:\n",
    "                log_response(call_id, \"Incomplete response or no 'done' flag found.\", 0, waiting_time)\n",
    "                return 0, waiting_time\n",
    "        else:\n",
    "            log_response(call_id, f\"Error: {response.status_code}\", 0, waiting_time)\n",
    "            return 0, waiting_time\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Calculate waiting_time if it hasn't been calculated yet\n",
    "        waiting_time = time.time() - start_call_time if waiting_time is None else waiting_time\n",
    "        log_response(call_id, f\"Request exception: {str(e)}\", 0, waiting_time)\n",
    "        return 0, waiting_time\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Calculate waiting_time if it hasn't been calculated yet\n",
    "        waiting_time = time.time() - start_call_time if waiting_time is None else waiting_time\n",
    "        log_response(call_id, f\"JSON decode exception: {str(e)}\", 0, waiting_time)\n",
    "        return 0, waiting_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_parallel_calls(n_calls):\n",
    "    start_time = time.time()\n",
    "    speeds = []\n",
    "    waiting_times = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_calls) as executor:\n",
    "        futures = [executor.submit(call_ollama_model, call_id) for call_id in range(n_calls)]\n",
    "        try:\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                speed, waiting_time = future.result()\n",
    "                if isinstance(speed, float):\n",
    "                    speeds.append(speed)\n",
    "                    waiting_times.append(waiting_time)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request exception: {str(e)}. Halting further execution.\")\n",
    "            return None, None, None  # Or raise an exception, or exit the script, depending on your requirements\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_speed = sum(speeds) / len(speeds) if speeds else 0\n",
    "    avg_waiting_time = sum(waiting_times) / len(waiting_times) if waiting_times else 0\n",
    "    return total_time / n_calls, avg_speed, avg_waiting_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    call_numbers = []\n",
    "    avg_times = []\n",
    "    avg_speeds = []\n",
    "    avg_waiting_times = []\n",
    "    simulation_failed = False\n",
    "\n",
    "    for n_calls in range(1, 20, 1):\n",
    "        try:\n",
    "            results = perform_parallel_calls(n_calls)\n",
    "            if results is None:  # If perform_parallel_calls returned None, it's a signal to stop the simulation\n",
    "                simulation_failed = True\n",
    "                break\n",
    "            avg_time, avg_speed, avg_waiting_time = results\n",
    "            call_numbers.append(n_calls)\n",
    "            avg_times.append(avg_time)\n",
    "            avg_speeds.append(avg_speed)\n",
    "            avg_waiting_times.append(avg_waiting_time)\n",
    "            print(f\"With {n_calls} parallel calls: Average time per call: {avg_time:.2f} seconds, \"\n",
    "                  f\"Average speed: {avg_speed:.2f} tokens per second, \"\n",
    "                  f\"Average waiting time: {avg_waiting_time:.2f} seconds.\")\n",
    "            time.sleep(2)  # Pause to avoid overwhelming the server\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            simulation_failed = True\n",
    "            break\n",
    "\n",
    "    # Plotting, executed regardless of simulation success or failure\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Stacked bar chart for call time and waiting time\n",
    "    ax1.bar(call_numbers, avg_waiting_times, label='Average Waiting Time', color='orange', width=0.4)\n",
    "    ax1.bar(call_numbers, avg_times, bottom=avg_waiting_times, label='Average Call Time', color='blue', width=0.4)\n",
    "    ax1.set_xlabel('Number of Calls')\n",
    "    ax1.set_ylabel('Time (seconds)')\n",
    "    ax1.tick_params(axis='y')\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Line chart for speed\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.plot(call_numbers, avg_speeds, label='Average Speed', color='green', marker='o')\n",
    "    ax2.set_ylabel('Speed (tokens/second)')\n",
    "    ax2.tick_params(axis='y')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Final layout touches\n",
    "    plt.title('Performance Metrics by Number of Calls')\n",
    "    fig.tight_layout()  # to ensure the right y-label is not clipped\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
